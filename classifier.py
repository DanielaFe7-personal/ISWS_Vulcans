# -*- coding: utf-8 -*-
"""Volcans_isws_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kx_fuxvQsbkOpXXvIW1cWqFHPIap6mhG
"""

# Commented out IPython magic to ensure Python compatibility.
import random
import urllib.request
import os
import glob

import pandas as pd
import numpy as np
import dateutil.parser
import datetime

import tensorflow as tf
import numpy as np
from itertools import cycle

from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from scipy import interp
from sklearn.metrics import roc_auc_score

# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
from google.colab import files
from keras.preprocessing import image
from tensorflow.keras.utils import load_img
from matplotlib import style

import numpy

# Check the root folder
!ls /content/drive/

# Path should be visually same as in drive.google.com
!ls /content/drive/MyDrive/ISWS_VOLCANS/

path_to_data = '/content/drive/MyDrive/ISWS_VOLCANS/'

def download_image(url,name,_class):
    fullname = path_to_data+"Figures/"+_class+str(name)+".jpg"
    try:
      urllib.request.urlretrieve(url,fullname)
    except:
      print("ERRORR ....")
      pass

labels = numpy.random.choice([1, 0], size=500, p=[.1, .9])
print(labels)
unos = 0
for i in labels:
  if i == 1:
    unos += 1
print(unos)

def collecting_url(path):
    "Removing images"
    files = glob.glob(path_to_data+"Figures/Positive/*")
    for f in files:
      os.remove(f)

    files = glob.glob(path_to_data+"Figures/Negative/*")
    for f in files:
      os.remove(f)

    df = pd.read_csv(path)
    _id = 0

    for url in df["image_url"].values[:500]:
        print(_id," ",url," ",labels[_id])
        if labels[_id] == 0:
          download_image(url,str(_id),"Negative/")
        else:
          download_image(url,str(_id),"Positive/")
        _id += 1

path = path_to_data+"output.csv"
#collecting_url(path)

#pip install split-folders
import splitfolders

files = glob.glob(path_to_data+"Data/train/Positive/*")
for f in files:
  os.remove(f)

files = glob.glob(path_to_data+"Data/train/Negative/*")
for f in files:
  os.remove(f)


files = glob.glob(path_to_data+"Data/val/Positive/*")
for f in files:
  os.remove(f)

files = glob.glob(path_to_data+"Data/val/Negative/*")
for f in files:
  os.remove(f)


files = glob.glob(path_to_data+"Data/test/Positive/*")
for f in files:
  os.remove(f)

files = glob.glob(path_to_data+"Data/test/Negative/*")
for f in files:
  os.remove(f)


splitfolders.ratio(path_to_data+"Figures/", # The location of dataset
                   output=path_to_data+"Data/", # The output location
                   seed=42, # The number of seed
                   ratio=(.7, .3), # The ratio of splited dataset
                   group_prefix=None, # If your dataset contains more than one file like ".jpg", ".pdf", etc
                   move=False # If you choose to move, turn this into True
                   )

# Parameters
dim1 = 64
dim2 = 64

train_positive = 0
train_negative = 0
files = glob.glob(path_to_data+"Data/train/Positive/*")
for f in files:
  train_positive += 1
files = glob.glob(path_to_data+"Data/train/Negative/*")
for f in files:
  train_negative += 1

print("Train data: ",train_positive+train_negative, " Positive: ",train_positive, " Negative: ", train_negative)

##

val_positive = 0
val_negative = 0
files = glob.glob(path_to_data+"Data/val/Positive/*")
for f in files:
  val_positive += 1
files = glob.glob(path_to_data+"Data/val/Negative/*")
for f in files:
  val_negative += 1

print("Val data: ",val_positive+val_negative, " Positive: ",val_positive, " Negative: ", val_negative)

##

test_positive = 0
test_negative = 0
files = glob.glob(path_to_data+"Data/test/Positive/*")
for f in files:
  test_positive += 1
files = glob.glob(path_to_data+"Data/test/Negative/*")
for f in files:
  test_negative += 1

print("Test data: ",test_positive+val_negative, " Positive: ",test_positive, " Negative: ", test_negative)

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# All images will be rescaled by 1./255
train_datagen = ImageDataGenerator(rescale=1/255)
validation_datagen = ImageDataGenerator(rescale=1/255)

# Flow training images in batches of 120 using train_datagen generator
train_generator = train_datagen.flow_from_directory(
        path_to_data+"Data/train",  # This is the source directory for training images
        classes = ['Positive', 'Negative'],
        target_size=(dim1, dim2),  # All images will be resized to 200x200
        batch_size=69,
        # Use binary labels
        class_mode='binary')

# Flow validation images in batches of 19 using valid_datagen generator
validation_generator = validation_datagen.flow_from_directory(
        path_to_data+"Data/val",  # This is the source directory for training images
        classes = ['Positive', 'Negative'],
        target_size=(dim1, dim2),  # All images will be resized to 200x200
        batch_size=32,
        # Use binary labels
        class_mode='binary',
        shuffle=True)

model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape = (dim1,dim2,3)),
                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])

model.summary()

model.compile(optimizer = tf.optimizers.Adam(),
              loss = 'binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_generator,
      steps_per_epoch=5,
      epochs=5,
      verbose=1,
      validation_data = validation_generator,
      validation_steps=3)

import keras
model.evaluate(validation_generator)
model.save(path_to_data+"Model/model")
model = keras.models.load_model(path_to_data+"Model/model")

plt.style.use('ggplot')
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

plt.style.use('ggplot')
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
from google.colab import files


files = glob.glob(path_to_data+"OURS/*")
for f in files:
  print(f)
  img = load_img(
    f,
    target_size=(dim1, dim2),
)

  img.show()
  x = tf.keras.utils.img_to_array(img, data_format=None, dtype=None)
  plt.imshow(x/255.)
  x = np.expand_dims(x, axis=0)
  images = np.vstack([x])
  classes = model.predict(images, batch_size=10)
  print(classes[0])
  if classes[0]<0.5:
    print(f + " is not a great art work :(")
  else:
    print(f + " is a great art work :)")

  print("*"*50)

